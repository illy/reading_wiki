The interval can also be used to assess the plausibility of a hypothesised value for the parameter.

It is intuitively reasonable to suggest that when a confidence interval (based on a sample) fails to include a certain value, then we should begin to doubt that the sample has been drawn from a population having a mean with that value.

__type I error__: it is due to rejecting the value (true population mean) when it is the correct value for the population mean.

> The probability of this type of error is exactly 5% chance that the true population mean accidentally falls outside the interval defined by the particular sample we have chosen.

__type II error__: although the population mean (true population mean) is no longer the value in type I error, that value is still included in the confidence interval.

> The probability of making this type of error will not be known.
Any attempt to protect ourselves against one type of error will increase the chance of making the other. The conventional way to solve the dilemma is to give more importance to the __type I error__.

The onus is on the investigator to show that some expected or natural hypothesis is untrue. Evidence for this should be accepted only if it is reasonably strong.

In some cases, the rejection of a hypothesis might lead to some costly action being taken, a change in educational procedures or extra help to some apparently disadvantaged section of the community.

__Test statistics__: The value Z = (x̄ - µ)/(s/√n) is used as the criterion to assess the degree to which the sample supports, or fails to support, the hypothesis that µ is the mean of the population from which the sample has been selected.

1. Every testable statistical hypothesis is judged on the basis of some test statistics derived from a sample.

2. Every test statistics will be a random variable because its value depends on the results of a random sample procedure.

3. In this case the distribution of our test statistics, Z, whenever µ is, in fact, the mean of the population from which the sample is taken, would be the standard normal distribution.

The hypothesis which is taken as a stating point and which often it is hoped will be refuted, is commonly called the null hypothesis and designated H0.

> The most important requirement to enable the test to take place is the existence of a suitable test statistic whose distribution is known when H0 is true.

__Alternative hypothesis__ (H1) is what we suspect may in fact be the case if the null hypothesis is untrue.

__Critical values of the test statistics__: values of the test statistic give support to H1 over H0.

The correct way to9 calculate the significance level of a test depends on the particular form of H1 which is considered relevant for the test and on the way in which the tables are presented.

A __two-tailed test__ is the appropriate one to use when the alternative hypothesis does not predict the direction of the difference.

A __one-tailed test__ is so called because values sufficiently far out in only one tail of the histogram of the test statistic will be taken as support for H1.

>A one-tailed test is appropriate when the direction of the difference is specified in the alternative hypothesis.

__Percentage points of the t-distribution__ are the values which will be exceeded in only such and such a per cent of samples if H0 is true.

> The percentage points in the table are relevant for a two-tailed test.

The significance level is just the probability that the null hypothesis will be rejected when it is correct.

The bigger the sample the smaller are the t-values which are found to be significant.

It is conventional to use the row corresponding to the nearest number of degrees of freedom smaller than those which are required when the latter are not tabulated.

A final point to notice about the t-table is that as the degrees of freedom increase the values become more and more similar to those of the standard normal distribution.

Every statistical test of hypothesis has a similar logic whatever are the hypotheses being tested.

1. There will be two hypotheses: the null must be precise while the other may be more of less vague;

2. There must be a test statistic whose distribution is known when the null is true. Percentage points of that distribution can be then be calculated and tabulated.

3. The major constraint on the use of significance tests is that it is generally difficult to discover test statistics with known properties.

__Important decision should not be taken simply on the basis of a statistical hypothesis test__.

> It is a misguided strategy to abandon an otherwise attractive line of research because a statistically significant result is not obtained as the result of a single experiment, or to believe that an unexpected rejection of a null hypothesis means, by itself, that an important scientific discovery has been made.

The emphasis is always on the type I error, error which arises when we incorrectly reject a true null hypothesis on the basis of this one statistical experiment. Sampling error, small sample size or the natural variability of the population under study may prevent us from detecting a substantial failure in the null hypothesis.

Hypothesis testing is set up as a conservative procedure, asking for a fairly small probability of a type I error before we make a serious claim that the null hypothesis has been refuted.

> If the experiment is repeated several times, whether by the same or by different investigators, the results need to be considered in a different light.

Another point to remember is that no null hypothesis will be exactly true.

The test statistic will have a large value if either the numerator is large or the denominator is small.

1. We can make the denominator as small as we please by increasing the value of n.

2. For very large values of n, the test statistic can be significant even if the difference between x̄ and µ is trivially small.
