Consequently, researchers as well as decision makers in companies and government institutions are forced to decide between two versions of the API: the freely-available but limited Streaming, and the very expensive but comprehensive Firehose version.

From a statistical point of view, the “law of large numbers” (mean of a sample converges to the mean of the entire population) and the Glivenko-Cantelli theorem (the unknown distribution X of an attribute in a population can be approximated with the observed distribution x) guarantee satisfactory results from sampled data when the randomly selected sub-sample is big enough.

One of the more interesting results in this dataset is that as the data in the Firehose spikes, the Streaming API coverage is reduced. One possible explanation for this phenomenon could be that due to the Western holidays observed at this time, activity on Twitter may have reduced causing the 1% threshold to go down.

In this period of time the Streaming API receives, on average, 43.5% of the data available on the Firehose on any given day. While this is much better than just 1% of the tweets promised by the Streaming API, we have no reference point for the data in the tweets we received.

Also, adding a hashtag to a tweet is equivalent to joining a community of users discussing the same topic (Yang et al. 2012).

In addition, hashtags are also used by Twitter to calculate the trending topics of the day, which encourages the user to post in these communities.

Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets.

Topic modeling is especially useful in large data, where it is too cumbersome to extract the topics manually.

Latent Dirichlet allocation is an algorithm for the automated discovery of topics. LDA treats documents as a mixture of topics, and topics as a mixture of words. Each topic discovered by LDA is represented by a probability distribution which conveys the affinity for a given word to that particular topic.

One way to mitigate this might be to create more specific parameter sets with different users, bounding boxes, and keywords. This way we might be able to extract more data from the Streaming API.

We find that the Streaming API data estimates the top hashtags for a large n well, but is often misleading when n is small.

The top hashtags for a large n well, but is often misleading when n is small.

We compare the probability distribution of the words from the most closely-matched topics and find that they are most similar when the coverage of the Streaming API is greatest.

We compare the statistical properties to find that the Streaming API performs worse than randomly sampled data, especially at low coverage.

We find that in the case of top hashtag analysis, the Streaming API sometimes reveals negative correlation in the top hashtags, while the randomly sampled data exhibits very high positive correlation with the Firehose data.

Surprisingly, we find that the Streaming API almost returns the complete set of the geotagged tweets despite sampling.

Overall, we find that the results of using the Streaming API depend strongly on the coverage and the type of analysis that the researcher wishes to perform.
