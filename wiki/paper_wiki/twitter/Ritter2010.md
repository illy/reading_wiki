Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”.

Dialogue act tagging has traditionally followed an annotatetrain-test paradigm, which begins with the design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004).

As an alternative solution for new media, we propose a series of unsupervised conversation models, where the discovery of acts amounts to clustering utterances with similar conversational roles.

This avoids manual construction of an act inventory, and allows the learning algorithm to tell us something about how people converse in a new medium.

In contrast to previous work, we address the problem of discovering dialogue acts in an informal, open-topic domain, where an unsupervised learner may be distracted by strong topic clusters.

In order to illustrate the spelling variation found on Twitter, we ran the Jcluster word clustering algorithm (Goodman, 2001) on our corpus, and manually picked out clusters of spelling variants; a sample is displayed in Table 1.

We first introduce the **summarization technology** we apply to this task, followed by two **Bayesian extensions**.

Our base model structure is inspired by the content model proposed by Barzilay and Lee (2004) for multi-document summarization.

Our goals are not so different: **we wish to discover the sequential dialogue structure of conversation**.

Rather than learning a disaster's location is followed by its death toll, we instead wish to learn that a question is followed by an answer.

During development, we found that **a unigram language model** performed best as the act emission distribution.

An initialconversation model can be created by simply applying the content modeling framework to conversation data. We rename the hidden states acts, and assume each post in a Twitter conversation is generated by a single act

The sequence model could instead partition entire conversations into topics, such as food, computers and music, and then predict that each topic self-transitions with high probability: if we begin talking about food, we are likely to continue to do so.

During development, we explored coarse methods to abstract away content while maintaining syntax, such as replacing tokens with either parts-of-speech or automatically-generated word clusters, but we found that these approaches degrade model performance.

Another approach to filtering out topic information leaves the data intact, but modifies the model to account for topic.

The goal of this extended model is to separate content words from dialogue indicators. Each word in a conversation is generated from one of three sources:

 >The current post's dialogue act • The conversation's topic • General English
Following LDA conventions, we place a symmetric Dirichlet prior over each of the multinomials. Dirichlet concentration parameters for act emission, act transition, conversation topic, general English, and source become the hyper-parameters of our model.

Therefore we adopt Gibbs sampling as our inference engine. Each hidden variable is sampled in turn, conditioned on a complete assignment of all other hidden variables through-out the data set.

There is reason to believe that integrating out multinomials and using sparse priors will improve the performance of the conversation model, as improvements have been observed when using a Bayesian HMM for unsupervised part-of-speech tagging (Goldwater and Griffiths, 2007).

Unlike previous work, our model automatically discovers an appropriate set of dialogue acts for a new medium; these acts will not necessarily have a close correspondence to dialogue act inventories manually designed for other corpora.

Instead of comparing against human annotations, we present a visualization of the automatically discovered dialogue acts, in addition to measuring the ability of our models to predict post order in unseen conversations.

Ideally we would evaluate performance using an end-use application such as a conversational agent; however as this is outside the scope of this paper, we leave such an evaluation to future work.

Although using multiple samples introduces the possibility of poor results due to “act drift”, we found this not to be a problem in practice; in fact, taking multiple samples substantially improved performance during development.

Note that our implementations can likely scale to larger data by using techniques such as SparseLDA (Yao et al., 2009). We limit our vocabulary to the 5,000 most frequent words in the corpus.

To visualize and interpret our competing models, we examined act-emission distributions, posts with high-confidence acts, and act-transition diagrams.

The Status act appears to represent a post in which the user is broadcasting information about what they are currently doing. This can be seen by the high amount of probability mass given to words like I and my, in addition to verbs such as go and get, as well as temporal nouns such as today, tomorrow and tonight.

The Status act generates the word I with high probability, whereas the likely response state Question generates you, followed by Response which again generates I.

The Kendall τ rank correlation coefficient measures the similarity between two permutations based on their agreement in pairwise orderings:

 >τ = (n+ − n−) / 2n

 >where n+ is the number of pairs that share the same order in both permutations, and n− is the number that do not.

In general, we found that using Bayesian inference outperforms EM. Also note that the Bayesian Conversation model outperforms the Conversation+Topic model at predicting conversation order.

This is likely because modelling conversation content as a sequence can in some cases help to predict post ordering; for example, adjacent posts are more likely to contain similar content words.

###Summary

Ritter, Cherry & Dolan (2010) regard the dialogue acts as an informal and open-topic domain, and use unsupervised learning to investigate the tweet data. They combine Jcluster word clustering algorithm and manual analysis to find out the various forms of spellings in tweets. Then, they use multi-document summarisation and bayesian extensions with LDA model to categorise the tweets according to different topic, and they find that the unigram model achieved the best accuracy. Their result shows that there is a large proportion of tweets relevant to users' status, and the first person pronoun, verbs go and get, and some temporal nouns such as today, tomorrow or tonight.
