With the large range of topics discussed on Twitter, it would be very difficult to manually collect enough data to train a sentiment classifier for tweets. Our solution is to use distant supervision, in which our training data consists of tweets with emoticons.

For the purposes of our research, we define **sentiment** to be “a personal positive or negative feeling.”

Many times it is unclear if a tweet contains a sentiment. For these cases, we use the following litmus test: If the tweet could ever appear as a frontpage newspaper headline or as a sentence in Wikipedia, then it belongs in the neutral class.

**In this research, we do not consider neutral tweets in our training or testing data**.

From our training set, we calculate that the average length of a tweet is 14 words or 78 characters.

The frequency of misspellings and slang in tweets is much higher than in other domains.

We strip the emoticons out from our training data. If we leave the emoticons in, there is a negative impact on the accuracies of the MaxEnt and SVM classifiers, but little effect on Naive Bayes. The difference lies in the mathematical models and feature weight selection of MaxEnt and SVM.

A *de facto standard* is to include the @ symbol before the username (e.g. @alecmgo).

The Twitter language model has many unique properties. We take advantage of the following properties to reduce the feature space.

Table 2 shows the effect of these '''feature reductions'''. These three reductions shrink the feature set down to 45.85% of its original size.

We use a multinomial Naive Bayes model. Class c∗ is assigned to tweet d, where

    c∗ = argmaccPNB(c|d)
    PNB(c|d) := (P(c)mi=1P(f|c)ni(d))/ P(d)

In this formula, f represents a feature and ni(d) represents the count of feature fi found in tweet d. There are a total of m features. Parameters P(c) and P(f|c) are obtained through maximum likelihood estimates, and add-1 smoothing is utilized for unseen features.

The idea behind Maximum Entropy models is that one should prefer the most uniform models that sastify a given constraint [7].

MaxEnt makes no independence assumptions for its features, unlike Naive Bayes. This means we can add features like bigrams and phrases to MaxEnt without worrying about features overlapping.

The model is represented by the following:

    PME(c|d, λ) = exp[Σiλifi(c, d)] / Σ''c''' exp[Σiλifi(c, d)]

In this formula, c is the class, d is the tweet, and λ is a weight vector. The weight vectors decide the significance of a feature in classification. A higher weight means that the feature is a strong indicator for the class. The weight vector is found by numerical optimization of the lambdas so as to maximize the conditional probability.

Theoretically, MaxEnt performs better than Naive Bayes be-cause it handles feature overlap better. However, in practice, Naive Bayes can still perform well on a variety of problems [7].

The Twitter API has a limit of 100 tweets in a response for any request. The scraper has a parameter that allows us to specify the frequency of polling. We found an interval of 2 minutes is a reasonable polling parameter.

1. Emoticons listed in Table 3 are stripped off. This is important for training purposes.

2. Any tweet containing both positive and negative emoticons are removed.

3. Retweets are removed.

4. Tweets with “:P” are removed.

 >These tweets are removed because “:P” usually does not imply a negative sentiment.

5. Repeated tweets are removed.

These results are very similar to Pang and Lee [9]. They report 81.0%, 80.4%, and 82.9% accuracy for Naive Bayes, MaxEnt, and SVM, respectively. This is very similar to our results of 81.3%, 80.5%, and 82.2% for the same set of classifiers.

In our experiments, negation as an explicit feature with unigrams does not improve accuracy, so we are very motivated to try bigrams.

 >However, bigrams tend to be very sparse and the overall accuracy drops in the case of both MaxEnt and SVM. Even collapsing the individual words to equivalence classes does not help.

In general using only bigrams as features is not useful because the feature space is very sparse. It is better to combine unigrams and bigrams as features.

We found that the POS tags were not useful. This is consistent with Pang and Lee [9]. The accuracy for Naive Bayes and SVM decreased while the performance for MaxEnt increased negligibly when compared to the unigram results.

Our algorithms classify the overall sentiment of a tweet. The polarity of a tweet may depend on the perspective you are interpreting the tweet from.

###Summary

Go, Bayanhi and huang (2009) test the Naive Bayes, MaxEnt, and SVMlight algorithms respectively on positive and negative tweets with the consideration of emoticons. Identifying properties of tweets such as username, URL link and repeated letters, they effectively do the feature reduction on the data. However they ignore the fundamental linguistic features of tweet as they only focus on the unigram and bigram features and suggest that the POS feature is no help of the overall performance.
