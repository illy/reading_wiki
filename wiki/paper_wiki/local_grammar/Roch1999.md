In language processing, finite-state models are not a lesser evil that bring simplicity and efficiency at the cost of accuracy . On the contrary they provide a very natural framework to describe complex linguistic phenomena.

However, finite-state modeling is usually thought as a necessary evil in the sense that more powerful formalisms such as context-free grammars are more accurate but of intractable size for reasonable efficiency .

One of the main drawback in using context-free grammars for modeling language is the inability or the difficult y of handling various types of deletion

 >(1) He expected John to buy a new book.

In the transformational grammar of Z. S. Harris (see [9] for instance), this sentence is analyzed as the verb expected (an operator) taking three arguments: (1) the subject he, (2) the first complement t John and (3) the sentence John buys a new book; the application of the verb operator deletes the subject of the last argument to transform the sentence into the infinitive clause to buy a new book.

This might not seem to be a difficult problem at first but recall that grammars have to be lexicalized (see [5, 2] for instance), that is, each rule should contain an explicit word, and therefore this type of duplication, which is only one duplication among many others, has to be repeated throughout the whole lexicon.

While other features of transducer parsing, such as

1.factorization, determinization and minimization of transducers can be used to generate more efficient parsers,

2.the grammar compilation, input sentences and the parsing use homogeneous representations,

3.parsing and building the grammar are the same operation, i.e. a rational transduction,

4.transforming a CFG into a transducer is yet another transduction,
j
Finite-state transducers (FST) are informally (see [1, 16] for formal definitions) finite-state automata for which each transition is labeled by a pair of labels.

A FST defines a function from strings in to strings in the following way: given an input sequence of symbols, if a path of the FST can be followed while reading the input sequence on the input label, then the concatenation of the output labels of the transitions of this path is an output for the input sequence.

Parsing will be considered here as a string transformation process.

Namely, if ∑g represents the set of symbols, such as (N, N) or (S, used to mark the syntactic analysis; if ∑w is the list of words in the language, then a parser is a mapping

    parser : ∑*w ➝ 2(∑g∙∑w∙∑g)*

One way of parsing context-free grammars with finite-state transducers consists of modeling a top-down analysis. Consider the sentence

The first step of building a parser consists of transforming each entry of this syntactic dictionary into a fnite-state transducer

The finite-state transducer related to an entry will be the machine responsible for analyzing a sentence with this structure.

Each transducer represents a transduction from ∑* to ∑* where ∑ = ∑w ⋃ ∑g.

Given the dictionary we define the grammar

    Tdic = ⋃(Ti ∈ dic)Ti as being the union of all the transducers defined in the dictionary

This transducer being given, parsing simply consists of applying the transducer on the input and checking whether the output is different from the input; if it is, then the transducer is applied again. The process is repeated until the input is not modified. Formally,

    paser = ∞Tdic

three categories of sentences have to be considered to build complete grammars.

1.The first category consists of free sentences like John eats potatoes,

2.the second one consists of sentences like John makes concessions, with complements with a smaller degree of variability. They are called support verb constructions.

3.Finally, the third category consists of frozen sentences, or idiomatic expressions,)
