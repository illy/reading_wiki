<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.0//EN"
    "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
      xmlns:dc="http://purl.org/dc/elements/1.1/"
      xmlns:foaf="http://xmlns.com/foaf/0.1/">
  
  <head>
    
    
      
        <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8" />
      
      
      <title>
        reading_wiki » 
        Chapter 2 Social media as corpora
      </title>
      
      
        <!-- YUI CSS reset, fonts, base -->
        <link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/combo?3.0.0/build/cssreset/reset-min.css&amp;3.0.0/build/cssfonts/fonts-min.css&amp;3.0.0/build/cssbase/base-min.css" media="screen, projection" />
        
        <link rel="stylesheet" type="text/css" href="../../media/css/style.css" media="screen, projection" />
        <link rel="stylesheet" type="text/css" href="../../media/css/pygments.css" media="screen, projection" />
      
      
      
      
      
        
      
    
  </head>
  
  <body >
    
      
      
        
          
  
    <ol id="breadcrumbs">
      
        <li class="crumb-0 not-last">
          
            <a href="../../">index</a>
          
        </li>
      
        <li class="crumb-1 not-last">
          
            <a href="../">book_wiki</a>
          
        </li>
      
        <li class="crumb-2 not-last">
          
            <a href="./">Zappavigna2012</a>
          
        </li>
      
        <li class="crumb-3 last">
          
            Zappavigna2012_02
          
        </li>
      
    </ol> <!-- ol#breadcrumbs -->
  

        
      
      
      <div id="content">
        
        
        
        <h1>Chapter 2 Social media as corpora</h1>
<p>Research into new media and the Internet from a communication perspective is diverse area that began to emerge <strong>around 1996</strong> (Tomasell et al. 209) (p15)</p>
<p>Studies of web-based discourse are a subdiscpline of <strong>CMC</strong> (computer-mediated communication). (p15)</p>
<p>Most often, these researcher will build their own corpora since CMC is <em>under-represented</em> in most available corpora. (p15)</p>
<p>A fundamental premise of corpus linguistics, equally applicable to web-based data as to more traditional data sources, is that corpora should be built according to carefully constructed selection criteria. (p15)</p>
<p>Unprocessed, the web is, however, not a corpus in the traditional sense, as it has not been built following selection criteria but instead has evolved as people have added, modified and deleted documents. (p16)</p>
<ol>
<li>
<p><em>Representativeness</em> is a concept used in statistics to refer to the extent to which a sample reflects the patterns in a larger population. (p16)</p>
</li>
<li>
<p><em>Balance refers</em> to the isssue that a corpus should contain an equi-distributed sample of the range of possible genres, were is possible to catalogue genres with any kind of exhaustive way. (p16)</p>
</li>
<li>
<p><em>Comparability</em> refers to the potential to compare different corpora by holding steady all parameters used for its construction except the one to be studied. (p16)</p>
</li>
</ol>
<p>Internet data can be highly 'noisy' in the sense that methods for automating text collection will retrieve results that were not intended by the linguist. (p16) In particular, most web documents will contain formatting and metadata that need some kind of processing before the main text can be included in a plain text file corpus or in a relational database that also stores metadata about the texts (p 16).</p>
<h3>Time and streaming data</h3>
<p>Time is of particular significance to online texts, particularly social media texts. Social media corpora are inevitably time-bound datasets, often shifting, networked assemblages of streaming data (p 16).</p>
<p>Streaming data is data that is produced as a sequence (p16). Streaming data produced with social media include unfolding status updates, such as microblog feeds or sequential blog posts, and feed of images and video (p 17).</p>
<p>Bernardini et al. (2006, p10) suggest four sense of web being as a corpus:</p>
<ol>
<li>
<p>The web as corpus surrogate;</p>
</li>
<li>
<p>The web as corpus shop;</p>
</li>
<li>
<p>The web as corpus proper;</p>
</li>
<li>
<p>The mega-corpus/mini-web (p17)</p>
</li>
</ol>
<p>Indeed, linguists fantasize abut a search engine that could filter internet data based on linguistic selection criteria and eliminate noise from the retrieved results (p17).</p>
<h2>Microposts as corpora</h2>
<p>Microblogging data is episodic in nature, with posts added to user's stream over time, often a frequent intervals (p18).</p>
<p>In this way the data is temporally bound, and the period in time over which a corpus of microposts is collected has significant impact on the properties of the corpus and the ways in which it can be used (p18).</p>
<p>Contextual phenomena such as high profile events, crises, seasons and holidays all have an impact on the kind of online talk occurring via this media. (p18)</p>
<p>While contextual variables intervene in all corpora, the impact appears to be particularly concentrated in micropost corpora, particularly on simple measures, such as word-frequency lists. (p18)</p>
<p>The variation of language over time is itself of intrinsic interest and the objective of studies that work with traditional diachronic corpora. (p18)</p>
<p>Within the domain of social media research, time is of particular interest to researchers studying patterns of trending topics (Cataldi et al. 2010). (p18)</p>
<p>This magnitude of data permits quantitative claims about the properties of social media networks to be made, such as the finding that topology analysis of the follower/following relationship shows 'a non-power-law follower distribution, a short effective diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks' (Kwak et al. 2010, p 600). (p19)</p>
<p>These types of textual features mean that it is not possible to use tools such as off-the-shelf POS taggers on micropost corpora without significant training of the tagger. (p19)</p>
<h3>Non-standard orthography</h3>
<p>Linguists can learn from the body of work in computational linguistics, where many of these types of text processing problems are encountered, when trying to automate various kinds of linguistic analyses. (p20)</p>
<h3>XML and escaped charcters</h3>
<p>The corpus linguist trying to work with tweets that have been scraped from an internet feed may encounter the trivial but real problem of escaped special characters if the input text has not been 'unescaped' properly. (p20)</p>
<p>If one of these characters occurs in the raw text of a tweet, it must be 'escaped' by being represented by another set of characters. (p20)</p>
<h3>Emoticons and hashtags</h3>
<p>Emoticons can pose a significant concordancing problem. Depending on the configuration of the concordance software used, some of the characters used in emoticons are not considered valid letters for that system and so will be filted out of the concordance. (p21)</p>
<h3>Abridge posts</h3>
<p>The result is that some tweets appear in an abridged form in the corpus. Alternatively, the tweet may contain punctuation indicating that it continues in a subsequent tweet in the stream. (p21)</p>
<p>However, these types of tweets are relatively uncommon and may be filtered out of the corpus if the analyst can identify regular syntactic patterns with which they can be identified and removed. (p21)</p>
<h3>The HERMES corpus</h3>
<p>Due to this randomization, much of the sequential nature of the streaming data is lost, and hence some other corpus collection strategy is required for exploring of exchanges between users. (p23)</p>
<p>Metadata about a tweet can at present be retrieved using the Twitter API. Linguists can make use of the API to build corpora taht contain different kinds of metadata retrieved from twitter. (p23)</p>
<p>High-level steps of collecting tweets:</p>
<ol>
<li>
<p>Download all the unfiltered tweets;</p>
</li>
<li>
<p>Separate the content of the tweets from other metadata;</p>
</li>
<li>
<p>Convert any entity sequence;</p>
</li>
<li>
<p>Filter the text so that it contains only English tweets. (p24)</p>
</li>
</ol>
<p>An in-depth tutorial on scraping data with the Twitter API is not provided here since the way Twitter allows developers to access its data and the extent of the granted access is changing as Twitter determines ways to generate income from the data. hence, any proposed method is likely to quickly become obsolete. (p24)</p>
        
        
        
        
        <hr class="clear" />
      </div> <!-- div#content -->
      
      
      <div id="footer">
        <p>
          
            reading_wiki —
          
          Powered by <a href="http://markdoc.org/">Markdoc</a>.
        </p>
      </div>
      
    
    
    
    <hr class="clear" />
  </body>
</html>